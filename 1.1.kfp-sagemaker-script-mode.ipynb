{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 1] 사용자 정의 스크립트 사용 (Bring Your Own Script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이 노브북을 실행하면 아래와 같은 결과를 보실 수 있습니다.\n",
    "- ![kf-pipeline.png](img/kf-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Components for Kubeflow Pipelines - script mode\n",
    "In this example we'll build a Kubeflow pipeline where every component call a different Amazon SageMaker feature.\n",
    "Our simple pipeline will perform:\n",
    "\n",
    "1. Hyperparameter optimization \n",
    "1. Select best hyperparameters and increase epochs\n",
    "1. Training model on the best hyperparameters \n",
    "1. Create an Amazon SageMaker model\n",
    "1. Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp import dsl\n",
    "import time, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm   = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training datasets and upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "Generating cifar10/train/train.tfrecords\n",
      "Generating cifar10/validation/validation.tfrecords\n",
      "Generating cifar10/eval/eval.tfrecords\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_folder      = 'jobs'\n",
    "dataset_folder  = 'datasets'\n",
    "local_dataset = 'cifar10'\n",
    "\n",
    "!python generate_cifar10_tfrecords.py --data-dir {local_dataset}\n",
    "datasets = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
    "\n",
    "# If dataset is already in S3 use the dataset's path:\n",
    "# datasets = 's3://{bucket_name}/{dataset_folder}/cifar10-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload training scripts to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./model_def.py\n",
      "./inference.py\n",
      "./cifar10-training-sagemaker.py\n",
      "./requirements.txt\n",
      "\n",
      "Uploaded to S3 location:\n",
      "s3://sagemaker-ap-northeast-2-790639055451/training-scripts/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz sourcedir.tar.gz --exclude=\".ipynb*\" -C code .\n",
    "source_s3 = sagemaker_session.upload_data(path='sourcedir.tar.gz', key_prefix='training-scripts')\n",
    "print('\\nUploaded to S3 location:')\n",
    "print(source_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom pipeline op\n",
    "Takes the results from a hyperparameter tuning job and increases the number of epochs for the next training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_best_model_hyperparams(hpo_results, best_model_epoch = \"80\") -> str:\n",
    "    import json\n",
    "    r = json.loads(str(hpo_results))\n",
    "    return json.dumps(dict(r,epochs=best_model_epoch))\n",
    "\n",
    "get_best_hyp_op = func_to_container_op(update_best_model_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 이미지 검색\n",
    "\n",
    "Available Deep Learning Containers Images\n",
    "- https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "특정 계정의 리파지토리 검색 명령어\n",
    "- `aws ecr describe-repositories --repository-names tensorflow-inference --registry-id 763104351884`\n",
    "\n",
    "특정 리파지토리의 Tag 검색\n",
    "- `aws ecr list-images --repository-name tensorflow-inference --registry-id 763104351884 --max-items 100 | grep 1.15.2-cpu`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:ap-northeast-2:763104351884:repository/tensorflow-inference\",\n",
      "            \"registryId\": \"763104351884\",\n",
      "            \"repositoryName\": \"tensorflow-inference\",\n",
      "            \"repositoryUri\": \"763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/tensorflow-inference\",\n",
      "            \"createdAt\": 1553139224.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! aws ecr describe-repositories --repository-names tensorflow-inference --registry-id 763104351884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            \"imageTag\": \"1.15.2-cpu-py36-ubuntu18.04-v4.4\"\n",
      "            \"imageTag\": \"1.15.2-cpu-py36-ubuntu18.04-v4.4-2020-03-19-21-33-20\"\n"
     ]
    }
   ],
   "source": [
    "! aws ecr list-images --repository-name tensorflow-inference --registry-id 763104351884 --max-items 100 | grep 1.15.2-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 이미지, 추론 이미지 설정\n",
    "**[중요] 리젼이 다르면(현재: 한국리젼) 이미지 경로를 수정해야 합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04\n",
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu\n"
     ]
    }
   ],
   "source": [
    "train_image='763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04'\n",
    "inference_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu'\n",
    "print(train_image)\n",
    "print(inference_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region:  ap-northeast-2\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "print(\"region: \", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='cifar10 hpo train deploy pipeline',\n",
    "    description='cifar10 hpo train deploy pipeline using sagemaker'\n",
    ")\n",
    "def cifar10_hpo_train_deploy(region=region,\n",
    "                           training_input_mode='File',\n",
    "                           train_image= train_image,\n",
    "                           serving_image= inference_image,                             \n",
    "                           volume_size='50',\n",
    "                           max_run_time='86400',\n",
    "                           instance_type='ml.p3.8xlarge',\n",
    "                           network_isolation='False',\n",
    "                           traffic_encryption='False',\n",
    "                           spot_instance='False',\n",
    "                           channels='[ \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"train\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/train\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"validation\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/validation\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"eval\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/eval\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    } \\\n",
    "                ]'\n",
    "                          ):\n",
    "    # Component 1\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy='Bayesian',\n",
    "        metric_name='val_acc',\n",
    "        metric_definitions='{\"val_acc\": \"val_acc: ([0-9\\\\\\\\.]+)\"}',\n",
    "        metric_type='Maximize',\n",
    "        static_parameters='{ \\\n",
    "            \"epochs\": \"1\", \\\n",
    "            \"momentum\": \"0.9\", \\\n",
    "            \"weight-decay\": \"0.0002\", \\\n",
    "            \"model_dir\":\"s3://'+bucket_name+'/jobs\", \\\n",
    "            \"sagemaker_program\": \"cifar10-training-sagemaker.py\", \\\n",
    "            \"sagemaker_region\": \"ap-northeast-2\", \\\n",
    "            \"sagemaker_submit_directory\": \"'+source_s3+'\" \\\n",
    "        }',\n",
    "        continuous_parameters='[ \\\n",
    "            {\"Name\": \"learning-rate\", \"MinValue\": \"0.0001\", \"MaxValue\": \"0.1\", \"ScalingType\": \"Logarithmic\"} \\\n",
    "        ]',\n",
    "        categorical_parameters='[ \\\n",
    "            {\"Name\": \"optimizer\", \"Values\": [\"sgd\", \"adam\"]}, \\\n",
    "            {\"Name\": \"batch-size\", \"Values\": [\"32\", \"128\", \"256\"]}, \\\n",
    "            {\"Name\": \"model-type\", \"Values\": [\"resnet\", \"custom\"]} \\\n",
    "        ]',\n",
    "        channels=channels,\n",
    "        output_location=f's3://{bucket_name}/jobs',\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs='1',\n",
    "        max_parallel_jobs='1',\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role\n",
    "    )\n",
    "    \n",
    "    # Component 2\n",
    "    training_hyp = get_best_hyp_op(hpo.outputs['best_hyperparameters'])\n",
    "    \n",
    "    # Component 3\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=training_hyp.output,\n",
    "        channels=channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=f's3://{bucket_name}/jobs',\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Component 4\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=serving_image,\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role\n",
    "    )\n",
    "\n",
    "    # Component 5\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1='ml.m5.large'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(cifar10_hpo_train_deploy,'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잡 확인\n",
    "- 현재 아래 셀의 결과로 나오는 링크는 동작 안합니다. 진행자가 안내하는 다른 URL을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/ab89e079-fe14-4af3-aec4-357cf6e3ad82\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/998e17e4-360f-4f1b-a1b4-eab69d0a10e1\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "aws_experiment = client.create_experiment(name='sm-kfp-experiment')\n",
    "\n",
    "exp_name    = f'cifar10-hpo-train-deploy-kfp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "my_run = client.run_pipeline(aws_experiment.id, exp_name, 'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 현재 추론은 에러 발생 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, numpy as np\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "file_name = '1000_dog.png'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName='Endpoint-20200522021801-DR5P', \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "pred = json.loads(response['Body'].read())['predictions']\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "for l,p in zip(labels, pred[0]):\n",
    "    print(l,\"{:.4f}\".format(p*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
