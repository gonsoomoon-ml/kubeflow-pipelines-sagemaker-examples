{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 스크립트 사용 (Bring Your Own Script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.4.0)\n",
      "Requirement already satisfied: fire>=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (11.0.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (1.4.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: Deprecated in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (1.2.12)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (1.37.1)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (1.6.0)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (1.28.0)\n",
      "Requirement already satisfied: strip-hints in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (0.1.7)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from fire>=0.3.1->kfp) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages (from fire>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (4.5)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (2.25.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (1.26.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (1.53.0)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (3.15.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (20.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.20)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.4)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2020.12.5)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from strip-hints->kfp) (0.36.2)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/dsl-compile\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp --upgrade\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Components for Kubeflow Pipelines - script mode\n",
    "In this example we'll build a Kubeflow pipeline where every component call a different Amazon SageMaker feature.\n",
    "Our simple pipeline will perform:\n",
    "\n",
    "1. Hyperparameter optimization \n",
    "1. Select best hyperparameters and increase epochs\n",
    "1. Training model on the best hyperparameters \n",
    "1. Create an Amazon SageMaker model\n",
    "1. Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp import dsl\n",
    "import time, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm   = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training datasets and upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "Generating cifar10/train/train.tfrecords\n",
      "Generating cifar10/validation/validation.tfrecords\n",
      "Generating cifar10/eval/eval.tfrecords\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_folder      = 'jobs'\n",
    "dataset_folder  = 'datasets'\n",
    "local_dataset = 'cifar10'\n",
    "\n",
    "!python generate_cifar10_tfrecords.py --data-dir {local_dataset}\n",
    "datasets = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
    "\n",
    "# If dataset is already in S3 use the dataset's path:\n",
    "# datasets = 's3://{bucket_name}/{dataset_folder}/cifar10-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload training scripts to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./inference.py\n",
      "./model_def.py\n",
      "./requirements.txt\n",
      "./cifar10-training-sagemaker.py\n",
      "\n",
      "Uploaded to S3 location:\n",
      "s3://sagemaker-ap-northeast-2-189546603447/training-scripts/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz sourcedir.tar.gz --exclude=\".ipynb*\" -C code .\n",
    "source_s3 = sagemaker_session.upload_data(path='sourcedir.tar.gz', key_prefix='training-scripts')\n",
    "print('\\nUploaded to S3 location:')\n",
    "print(source_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom pipeline op\n",
    "Takes the results from a hyperparameter tuning job and increases the number of epochs for the next training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_best_model_hyperparams(hpo_results, best_model_epoch = \"80\") -> str:\n",
    "    import json\n",
    "    r = json.loads(str(hpo_results))\n",
    "    return json.dumps(dict(r,epochs=best_model_epoch))\n",
    "\n",
    "get_best_hyp_op = func_to_container_op(update_best_model_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04\n",
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu\n"
     ]
    }
   ],
   "source": [
    "train_image='763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04'\n",
    "inference_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu'\n",
    "print(train_image)\n",
    "print(inference_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='cifar10 hpo train deploy pipeline',\n",
    "    description='cifar10 hpo train deploy pipeline using sagemaker'\n",
    ")\n",
    "def cifar10_hpo_train_deploy(region='ap-northeast-2',\n",
    "                           training_input_mode='File',\n",
    "#                            train_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04',\n",
    "#                            serving_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu',\n",
    "                           train_image= train_image,\n",
    "                           serving_image= inference_image,                             \n",
    "                           volume_size='50',\n",
    "                           max_run_time='86400',\n",
    "                           instance_type='ml.p3.2xlarge',\n",
    "                           network_isolation='False',\n",
    "                           traffic_encryption='False',\n",
    "                           spot_instance='False',\n",
    "                           channels='[ \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"train\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/train\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"validation\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/validation\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"eval\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/eval\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    } \\\n",
    "                ]'\n",
    "                          ):\n",
    "    # Component 1\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy='Bayesian',\n",
    "        metric_name='val_acc',\n",
    "        metric_definitions='{\"val_acc\": \"val_acc: ([0-9\\\\\\\\.]+)\"}',\n",
    "        metric_type='Maximize',\n",
    "        static_parameters='{ \\\n",
    "            \"epochs\": \"1\", \\\n",
    "            \"momentum\": \"0.9\", \\\n",
    "            \"weight-decay\": \"0.0002\", \\\n",
    "            \"model_dir\":\"s3://'+bucket_name+'/jobs\", \\\n",
    "            \"sagemaker_program\": \"cifar10-training-sagemaker.py\", \\\n",
    "            \"sagemaker_region\": \"ap-northeast-2\", \\\n",
    "            \"sagemaker_submit_directory\": \"'+source_s3+'\" \\\n",
    "        }',\n",
    "        continuous_parameters='[ \\\n",
    "            {\"Name\": \"learning-rate\", \"MinValue\": \"0.0001\", \"MaxValue\": \"0.1\", \"ScalingType\": \"Logarithmic\"} \\\n",
    "        ]',\n",
    "        categorical_parameters='[ \\\n",
    "            {\"Name\": \"optimizer\", \"Values\": [\"sgd\", \"adam\"]}, \\\n",
    "            {\"Name\": \"batch-size\", \"Values\": [\"32\", \"128\", \"256\"]}, \\\n",
    "            {\"Name\": \"model-type\", \"Values\": [\"resnet\", \"custom\"]} \\\n",
    "        ]',\n",
    "        channels=channels,\n",
    "        output_location=f's3://{bucket_name}/jobs',\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs='1',\n",
    "        max_parallel_jobs='1',\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role\n",
    "    )\n",
    "    \n",
    "    # Component 2\n",
    "    training_hyp = get_best_hyp_op(hpo.outputs['best_hyperparameters'])\n",
    "    \n",
    "    # Component 3\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=training_hyp.output,\n",
    "        channels=channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=f's3://{bucket_name}/jobs',\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Component 4\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=serving_image,\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role\n",
    "    )\n",
    "\n",
    "    # Component 5\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1='ml.m5.large'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(cifar10_hpo_train_deploy,'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/70109ec1-eda3-4912-a073-d92b66800964\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/83524d7c-637b-4a39-90fb-73e46f9a9484\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "aws_experiment = client.create_experiment(name='sm-kfp-experiment')\n",
    "\n",
    "exp_name    = f'cifar10-hpo-train-deploy-kfp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "my_run = client.run_pipeline(aws_experiment.id, exp_name, 'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, numpy as np\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "file_name = '1000_dog.png'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName='Endpoint-20200522021801-DR5P', \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "pred = json.loads(response['Body'].read())['predictions']\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "for l,p in zip(labels, pred[0]):\n",
    "    print(l,\"{:.4f}\".format(p*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
